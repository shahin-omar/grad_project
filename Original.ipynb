import pandas as pd
import numpy as np
import statsmodels.api as sm
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.model_selection import RepeatedKFold
from keras.models import Sequential
from keras.layers import Dense
from itertools import product

df=pd.read_csv("Data Input.csv")
df.head()

df1=df.drop(['Exit'],axis=1)
X_Entry = df1.drop(['Entry'],axis=1)
Y_Entry = df1['Entry']


df2=df.drop(['Entry'],axis=1)
X_Exit = df2.drop(['Exit'],axis=1)
Y_Exit = df2['Exit']

#Dividing the data into test and train
X_entry_train, X_entry_test, y_entry_train, y_entry_test = train_test_split(X_Entry, Y_Entry, test_size=0.33)
X_exit_train, X_exit_test, y_exit_train, y_exit_test = train_test_split(X_Exit, Y_Exit, test_size=0.33)

#Polynomial regression
def polyreg(X,Y):
  X1 = sm.add_constant(X)
  model= sm.OLS(Y, X1).fit()
  print(model.summary())
  y_pred=model.predict(X1)
  print("Error is: {}".format(mean_squared_error(Y,y_pred)))
  X['Actual']=Y
  X['Predicted']=y_pred
  return X

reg_entry = polyreg(X_Entry,Y_Entry)
reg_entry




reg_exit=polyreg(X_Exit,Y_Exit)
reg_exit

def randomreg(X,Y):
  X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33,random_state = 0)
  scaler = StandardScaler()
  scaler.fit(X_train)
  X_train = scaler.transform(X_train)
  X_test = scaler.transform(X_test)
  estimators = np.arange(10, 200, 10)
  scores = []
  for n in estimators:
    model = RandomForestRegressor(n_estimators = n).fit(X_train,y_train)
    scores.append(model.score(X_test, y_test))
  model = RandomForestRegressor(n_estimators = 200).fit(X_train,y_train)
  y_pred = model.predict(X)
  X['Actual']=Y
  X['Predicted']=y_pred

  plt.title("Effect of n_estimators")
  plt.xlabel("n_estimator")
  plt.ylabel("score")
  plt.plot(estimators, scores)
  print("Error is: {}".format(mean_squared_error(y_test,model.predict(X_test))))
  return model,X   



model_entry,random_entry=randomreg(X_Entry,Y_Entry)
random_entry

model_exit,random_exit=randomreg(X_Exit,Y_Exit)
random_exit

def gbtree(X,Y):
  X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33,random_state = 0)
  params={'n_estimators':3,'max_depth':3,'learning_rate':1,'criterion':'mse'}
  model = GradientBoostingRegressor(**params).fit(X_train,y_train)
  y_pred = model.predict(X)
  X['Actual']=Y
  X['Predicted']=y_pred
  print("Error is: {}".format(mean_squared_error(y_test,model.predict(X_test))))
  return X

gbt_entry=gbtree(X_Entry,Y_Entry)
gbt_entry

gbt_exit=gbtree(X_Exit,Y_Exit)
gbt_exit

def adareg(X,Y):
  X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33,random_state = 0)
  model = AdaBoostRegressor().fit(X_train,y_train)
  y_pred = model.predict(X)
  X['Actual']=Y
  X['Predicted']=y_pred
  print("Error is: {}".format(mean_squared_error(y_test,model.predict(X_test))))
  #return X
  return model,X

#ada_entry=adareg(X_Entry,Y_Entry)
#ada_entry
model_ada , ada_entry = adareg(X_Entry,Y_Entry)
ada_entry



ada_exit=adareg(X_Exit,Y_Exit)
ada_exit

#this is for nerual network
def get_model(n_inputs, n_outputs):
	model = Sequential()
	model.add(Dense(n_inputs/2, input_dim=n_inputs, kernel_initializer='he_uniform', activation='relu'))
	model.add(Dense(n_outputs))
	model.compile(loss='mae', optimizer='adam',metrics=['accuracy'])
	return model

def evaluate_model(X, y,nsplits,nrepeats,epochs):
	results = list()
	
	if y.ndim==1:
		n_inputs, n_outputs = X.shape[1], 1
	else:
		n_inputs, n_outputs = X.shape[1], y.shape[1]
	# define evaluation procedure
	cv = RepeatedKFold(n_splits=nsplits, n_repeats=nrepeats, random_state=1)
	# enumerate folds
	for train_ix, test_ix in cv.split(X):
		X_train, X_test = X[train_ix], X[test_ix]
		y_train, y_test = y[train_ix], y[test_ix]
		model = get_model(n_inputs, n_outputs)
		model_fin=model.fit(X_train, y_train, epochs=epochs, batch_size=1, validation_data=(X_test, y_test))
		#print('>%.3f' % mae)
	#Create plot
	loss_train = model_fin.history['loss']
	loss_val = model_fin.history['val_loss']
	epochs = range(1,epochs+1)
	plt.plot(epochs, loss_train, 'g', label='Training loss')
	plt.plot(epochs, loss_val, 'b', label='validation loss')
	plt.title('Training and Validation loss')
	plt.xlabel('Epochs')
	plt.ylabel('Loss')
	plt.legend()
	plt.show()

	pred=model.predict(X_test)
	return model,pred,y_test

X=np.array(df.drop(['Entry','Exit'],axis=1))
Y=np.array(df[['Entry','Exit']])
evaluate_model(X,Y,4,3,100)

def expand_grid(dictionary):
   return pd.DataFrame([row for row in product(*dictionary.values())], 
                       columns=dictionary.keys())
   
dictionary = {'Drill Bit Diameter': np.linspace(6,7,100), 
              'Spindle Speed': np.linspace(3600,3700,200), 
              'Feed Rate': np.linspace(160,170,100)}

new_testdf=expand_grid(dictionary)

y_pred_entry=model_entry.predict(new_testdf)
y_pred_exit=model_exit.predict(new_testdf)
new_testdf['Entry_Predictions']=y_pred_entry
new_testdf['Exit_Predictions']=y_pred_exit
(new_testdf.sort_values("Entry_Predictions",ascending=True)).head()


import seaborn as sns
sns.lineplot(x="Drill Bit Diameter",y="Entry_Predictions",data=new_testdf)

(new_testdf.sort_values("Exit_Predictions",ascending=True)).head()
